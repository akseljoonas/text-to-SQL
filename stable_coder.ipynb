{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1035 [02:33<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:32<00:00, 16.35s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('json', data_files='/Users/mikaumana/Downloads/dataset.json')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stable-code-3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"stabilityai/stable-code-3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_id': 0,\n",
       " 'db_id': 'california_schools',\n",
       " 'question': 'What is the highest eligible free rate for K-12 students in the schools in Alameda County?',\n",
       " 'evidence': 'Eligible free rate for K-12 = `Free Meal Count (K-12)` / `Enrollment (K-12)`',\n",
       " 'SQL': \"SELECT `Free Meal Count (K-12)` / `Enrollment (K-12)` FROM frpm WHERE `County Name` = 'Alameda' ORDER BY (CAST(`Free Meal Count (K-12)` AS REAL) / `Enrollment (K-12)`) DESC LIMIT 1\",\n",
       " 'difficulty': 'simple',\n",
       " 'database_schema': 'CREATE TABLE `frpm` (\\n  CDSCode TEXT PRIMARY KEY REFERENCES schools(CDSCode),\\n  Academic Year TEXT,\\n  County Code TEXT,\\n  District Code INTEGER,\\n  School Code TEXT,\\n  County Name TEXT,\\n  District Name TEXT,\\n  School Name TEXT,\\n  District Type TEXT,\\n  School Type TEXT,\\n  Educational Option Type TEXT,\\n  NSLP Provision Status TEXT,\\n  Charter School (Y/N) INTEGER,\\n  Charter School Number TEXT,\\n  Charter Funding Type TEXT,\\n  IRC INTEGER,\\n  Low Grade TEXT,\\n  High Grade TEXT,\\n  Enrollment (K-12) REAL,\\n  Free Meal Count (K-12) REAL,\\n  Percent (%) Eligible Free (K-12) REAL,\\n  FRPM Count (K-12) REAL,\\n  Percent (%) Eligible FRPM (K-12) REAL,\\n  Enrollment (Ages 5-17) REAL,\\n  Free Meal Count (Ages 5-17) REAL,\\n  Percent (%) Eligible Free (Ages 5-17) REAL,\\n  FRPM Count (Ages 5-17) REAL,\\n  Percent (%) Eligible FRPM (Ages 5-17) REAL,\\n  2013-14 CALPADS Fall 1 Certification Status INTEGER\\n);\\n\\nCREATE TABLE `satscores` (\\n  cds TEXT PRIMARY KEY REFERENCES schools(CDSCode),\\n  rtype TEXT,\\n  sname TEXT,\\n  dname TEXT,\\n  cname TEXT,\\n  enroll12 INTEGER,\\n  NumTstTakr INTEGER,\\n  AvgScrRead INTEGER,\\n  AvgScrMath INTEGER,\\n  AvgScrWrite INTEGER,\\n  NumGE1500 INTEGER\\n);\\n\\nCREATE TABLE `schools` (\\n  CDSCode TEXT PRIMARY KEY,\\n  NCESDist TEXT,\\n  NCESSchool TEXT,\\n  StatusType TEXT,\\n  County TEXT,\\n  District TEXT,\\n  School TEXT,\\n  Street TEXT,\\n  StreetAbr TEXT,\\n  City TEXT,\\n  Zip TEXT,\\n  State TEXT,\\n  MailStreet TEXT,\\n  MailStrAbr TEXT,\\n  MailCity TEXT,\\n  MailZip TEXT,\\n  MailState TEXT,\\n  Phone TEXT,\\n  Ext TEXT,\\n  Website TEXT,\\n  OpenDate DATE,\\n  ClosedDate DATE,\\n  Charter INTEGER,\\n  CharterNum TEXT,\\n  FundingType TEXT,\\n  DOC TEXT,\\n  DOCType TEXT,\\n  SOC TEXT,\\n  SOCType TEXT,\\n  EdOpsCode TEXT,\\n  EdOpsName TEXT,\\n  EILCode TEXT,\\n  EILName TEXT,\\n  GSoffered TEXT,\\n  GSserved TEXT,\\n  Virtual TEXT,\\n  Magnet INTEGER,\\n  Latitude REAL,\\n  Longitude REAL,\\n  AdmFName1 TEXT,\\n  AdmLName1 TEXT,\\n  AdmEmail1 TEXT,\\n  AdmFName2 TEXT,\\n  AdmLName2 TEXT,\\n  AdmEmail2 TEXT,\\n  AdmFName3 TEXT,\\n  AdmLName3 TEXT,\\n  AdmEmail3 TEXT,\\n  LastUpdate DATE\\n);'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train'].train_test_split(test_size=0.1)[\"train\"]\n",
    "eval_dataset = dataset['train'].train_test_split(test_size=0.1)[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(train_dataset):\n",
    "  output_texts = []\n",
    "  for i in range(len(train_dataset['question'])):\n",
    "    question = train_dataset['question'][i]\n",
    "    query = train_dataset['SQL'][i]\n",
    "    database_schema = train_dataset['database_schema'][i]\n",
    "    user_message = f\"\"\"Given the following SQL tables, your job is to generate the Sqlite SQL query given the user's question.\n",
    "Put your answer inside the ```sql and ``` tags.\n",
    "{database_schema}\n",
    "###\n",
    "Question: {question}\n",
    "\n",
    "```sql\n",
    "{query} ;\n",
    "```\n",
    "<|EOT|>\n",
    "\"\"\"\n",
    "    output_texts.append(user_message)\n",
    "  return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = formatting_prompts_func(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_prompts = []\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "for row in x:\n",
    "    inputs = tokenizer(row, padding=\"max_length\", truncation=True,  max_length=512, return_tensors=\"pt\").to(model.device)\n",
    "    tokenized_prompts.append(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[15768,   253,  1563, 11700,  7180,    13,   634,  2628,   310,   281,\n",
       "          6635,   253, 39542,   614, 11700,  7316,  1677,   253,  2608,   434,\n",
       "          1953,    15,   187, 12501,   634,  3662,  3304,   253,  2634,  5190,\n",
       "         11296,   285,  2634,  5190, 14610,    15,   187, 25875, 22454,  2634,\n",
       "           925,  2617,    65,   313,   187, 50276,  3717,  4061,   853, 46434,\n",
       "          4653, 44825, 25111,  5689, 38064, 20712,  1410,  6629,     9,  3717,\n",
       "          4061,   853,   582,   187, 50276, 42228,   280, 10519, 46434,    13,\n",
       "           187, 50276, 43582,  6307, 46434,    13,   187, 50276, 31711,  6307,\n",
       "         41847,    13,   187, 50276, 28014,  6307, 46434,    13,   187, 50276,\n",
       "         43582,  9424, 46434,    13,   187, 50276, 31711,  9424, 46434,    13,\n",
       "           187, 50276, 28014,  9424, 46434,    13,   187, 50276, 31711,  8078,\n",
       "         46434,    13,   187, 50276, 28014,  8078, 46434,    13,   187, 50276,\n",
       "         25735,  1050, 27357,  8078, 46434,    13,   187, 50276,  4883, 13010,\n",
       "          1294,  4694, 20364, 46434,    13,   187, 50276,  1779,  5179,  4726,\n",
       "           313,    58,    16,    47,    10, 41847,    13,   187, 50276,  1779,\n",
       "          5179,  4726, 11057, 46434,    13,   187, 50276,  1779,  5179, 44303,\n",
       "          8078, 46434,    13,   187, 50276,  3027,    36, 41847,    13,   187,\n",
       "         50276, 20357, 28775, 46434,    13,   187, 50276, 12412, 28775, 46434,\n",
       "            13,   187, 50276,  3546,  1811,   420,   313,    44,    14,   805,\n",
       "            10, 40830,    13,   187, 50276, 14344,  3189,   267,  8240,   313,\n",
       "            44,    14,   805,    10, 40830,    13,   187, 50276, 34878, 11767,\n",
       "           444,  3372,   917,  7648,   313,    44,    14,   805,    10, 40830,\n",
       "            13,   187, 50276,  6764,  9122,  8240,   313,    44,    14,   805,\n",
       "            10, 40830,    13,   187, 50276, 34878, 11767,   444,  3372,   917,\n",
       "          7499,  9122,   313,    44,    14,   805,    10, 40830,    13,   187,\n",
       "         50276,  3546,  1811,   420,   313,    34,  2510,   608,    14,  1166,\n",
       "            10, 40830,    13,   187, 50276, 14344,  3189,   267,  8240,   313,\n",
       "            34,  2510,   608,    14,  1166,    10, 40830,    13,   187, 50276,\n",
       "         34878, 11767,   444,  3372,   917,  7648,   313,    34,  2510,   608,\n",
       "            14,  1166,    10, 40830,    13,   187, 50276,  6764,  9122,  8240,\n",
       "           313,    34,  2510,   608,    14,  1166,    10, 40830,    13,   187,\n",
       "         50276, 34878, 11767,   444,  3372,   917,  7499,  9122,   313,    34,\n",
       "          2510,   608,    14,  1166,    10, 40830,    13,   187, 50276,  6622,\n",
       "            14,  1047, 30865,    49, 36160, 16275,   337, 19937,  1877, 20364,\n",
       "         41847,   187,   558,   187,   187, 25875, 22454,  2634, 22354, 44142,\n",
       "            65,   313,   187, 50276,    68,  1397, 46434,  4653, 44825, 25111,\n",
       "          5689, 38064, 20712,  1410,  6629,     9,  3717,  4061,   853,   582,\n",
       "           187, 50276,    83,   881, 46434,    13,   187, 50276,    84,  1590,\n",
       "         46434,    13,   187, 50276,    69,  1590, 46434,    13,   187, 50276,\n",
       "            68,  1590, 46434,    13,   187, 50276,   257,  1811,   805, 41847,\n",
       "            13,   187, 50276, 12753,    53,   296, 39520,    83, 41847,    13,\n",
       "           187, 50276, 13834,    72,  4316,    83,  5703, 41847,    13,   187,\n",
       "         50276, 13834,    72,  4316,    83, 18102, 41847,    13,   187, 50276,\n",
       "         13834,    72,  4316,    83, 10639, 41847,    13,   187, 50276, 12753,\n",
       "          7538, 33856, 41847,   187,   558,   187,   187, 25875, 22454,  2634,\n",
       "         19221,    84,    65,   313,   187, 50276,  3717,  4061,   853, 46434,\n",
       "          4653, 44825, 25111,    13,   187, 50276,  9431,  1410, 14178, 46434,\n",
       "            13,   187, 50276,  9431,  5479,  1651, 46434,    13,   187, 50276,\n",
       "          9505,  2548, 46434,    13,   187, 50276, 43582, 46434,    13,   187,\n",
       "         50276, 31711, 46434,    13,   187, 50276, 28014, 46434,    13,   187,\n",
       "         50276, 42992, 46434,    13,   187, 50276, 42992,    34,  1288, 46434,\n",
       "            13,   187]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # Output directory\n",
    "    eval_strategy=\"epoch\",     # Evaluation strategy (per epoch)\n",
    "    learning_rate=5e-5,              # Learning rate\n",
    "    per_device_train_batch_size=4,   # Batch size for training\n",
    "    per_device_eval_batch_size=4,    # Batch size for evaluation\n",
    "    num_train_epochs=3,              # Number of epochs\n",
    "    weight_decay=0.01,               # Weight decay\n",
    "    logging_dir=\"./logs\",            # Directory for logs\n",
    "    save_total_limit=3,              # Number of model checkpoints to save\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_prompts,  # Training dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1035 [05:19<?, ?it/s]\n",
      "  0%|          | 0/1035 [03:31<?, ?it/s]\n",
      "  0%|          | 0/1035 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[4, 4, 1, 1, 512]}, size=[4, 1, 1, 512]): the number of sizes provided (4) must be greater or equal to the number of dimensions in the tensor (5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/code_llama_env/lib/python3.9/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2053\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   2054\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   2055\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   2056\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   2057\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/code_llama_env/lib/python3.9/site-packages/transformers/trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2385\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   2387\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2388\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   2390\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2393\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/code_llama_env/lib/python3.9/site-packages/transformers/trainer.py:3485\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3482\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   3484\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3485\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   3487\u001b[0m \u001b[39mdel\u001b[39;00m inputs\n\u001b[1;32m   3488\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   3489\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtorch_empty_cache_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3490\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtorch_empty_cache_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   3491\u001b[0m ):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/code_llama_env/lib/python3.9/site-packages/transformers/trainer.py:3532\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3530\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3531\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 3532\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   3533\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3534\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3535\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/code_llama_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/code_llama_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/code_llama_env/lib/python3.9/site-packages/transformers/models/stablelm/modeling_stablelm.py:1219\u001b[0m, in \u001b[0;36mStableLmForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   1214\u001b[0m output_hidden_states \u001b[39m=\u001b[39m (\n\u001b[1;32m   1215\u001b[0m     output_hidden_states \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1216\u001b[0m )\n\u001b[1;32m   1217\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1219\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1220\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1221\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1222\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1223\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1224\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1225\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1226\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1227\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1228\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1229\u001b[0m     cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m   1230\u001b[0m )\n\u001b[1;32m   1232\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1233\u001b[0m \u001b[39m# No upscaling to float was ever done for StableLm\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/code_llama_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/code_llama_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/code_llama_env/lib/python3.9/site-packages/transformers/models/stablelm/modeling_stablelm.py:989\u001b[0m, in \u001b[0;36mStableLmModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[39mif\u001b[39;00m position_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    987\u001b[0m     position_ids \u001b[39m=\u001b[39m cache_position\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 989\u001b[0m causal_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_causal_mask(\n\u001b[1;32m    990\u001b[0m     attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n\u001b[1;32m    991\u001b[0m )\n\u001b[1;32m    993\u001b[0m hidden_states \u001b[39m=\u001b[39m inputs_embeds\n\u001b[1;32m    995\u001b[0m \u001b[39m# create position embeddings to be shared across the decoder layers\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/code_llama_env/lib/python3.9/site-packages/transformers/models/stablelm/modeling_stablelm.py:1101\u001b[0m, in \u001b[0;36mStableLmModel._update_causal_mask\u001b[0;34m(self, attention_mask, input_tensor, cache_position, past_key_values, output_attentions)\u001b[0m\n\u001b[1;32m   1094\u001b[0m     target_length \u001b[39m=\u001b[39m (\n\u001b[1;32m   1095\u001b[0m         attention_mask\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m   1096\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(attention_mask, torch\u001b[39m.\u001b[39mTensor)\n\u001b[1;32m   1097\u001b[0m         \u001b[39melse\u001b[39;00m past_seen_tokens \u001b[39m+\u001b[39m sequence_length \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1098\u001b[0m     )\n\u001b[1;32m   1100\u001b[0m \u001b[39m# In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\u001b[39;00m\n\u001b[0;32m-> 1101\u001b[0m causal_mask \u001b[39m=\u001b[39m _prepare_4d_causal_attention_mask_with_cache_position(\n\u001b[1;32m   1102\u001b[0m     attention_mask,\n\u001b[1;32m   1103\u001b[0m     sequence_length\u001b[39m=\u001b[39;49msequence_length,\n\u001b[1;32m   1104\u001b[0m     target_length\u001b[39m=\u001b[39;49mtarget_length,\n\u001b[1;32m   1105\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1106\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m   1107\u001b[0m     min_dtype\u001b[39m=\u001b[39;49mmin_dtype,\n\u001b[1;32m   1108\u001b[0m     cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m   1109\u001b[0m     batch_size\u001b[39m=\u001b[39;49minput_tensor\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m   1110\u001b[0m )\n\u001b[1;32m   1112\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1113\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_attn_implementation \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msdpa\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1114\u001b[0m     \u001b[39mand\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[39m# using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[39m# Details: https://github.com/pytorch/pytorch/issues/110213\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     causal_mask \u001b[39m=\u001b[39m AttentionMaskConverter\u001b[39m.\u001b[39m_unmask_unattended(causal_mask, min_dtype)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/code_llama_env/lib/python3.9/site-packages/transformers/models/stablelm/modeling_stablelm.py:109\u001b[0m, in \u001b[0;36m_prepare_4d_causal_attention_mask_with_cache_position\u001b[0;34m(attention_mask, sequence_length, target_length, dtype, device, min_dtype, cache_position, batch_size)\u001b[0m\n\u001b[1;32m    107\u001b[0m         padding_mask \u001b[39m=\u001b[39m causal_mask[:, :, :, :mask_length] \u001b[39m+\u001b[39m attention_mask[:, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, :]\n\u001b[1;32m    108\u001b[0m         padding_mask \u001b[39m=\u001b[39m padding_mask \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 109\u001b[0m         causal_mask[:, :, :, :mask_length] \u001b[39m=\u001b[39m causal_mask[:, :, :, :mask_length]\u001b[39m.\u001b[39mmasked_fill(\n\u001b[1;32m    110\u001b[0m             padding_mask, min_dtype\n\u001b[1;32m    111\u001b[0m         )\n\u001b[1;32m    113\u001b[0m \u001b[39mreturn\u001b[39;00m causal_mask\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[4, 4, 1, 1, 512]}, size=[4, 1, 1, 512]): the number of sizes provided (4) must be greater or equal to the number of dimensions in the tensor (5)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.20 ('code_llama_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8eb9eafc516786d496df4f2e03b26aea56404ce31a6cb982c368f1ada82447b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
