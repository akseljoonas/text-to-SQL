{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full-Parameter Fine-Tuning Stable-Code 3B on Text-to-SQL task on the BIRD train dataset and evaluating it on mini-dev dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!module load CUDA\n",
    "!module load cuDNN/8.9.2.26-CUDA-12.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -y torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch==2.4.0 --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.4.0+cu121\n",
      "CUDA Version: 12.1\n",
      "CUDA Available: True\n",
      "Number of GPUs: 1\n",
      "Current CUDA Device: 0\n",
      "Device Name: NVIDIA A100-PCIE-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"Current CUDA Device:\", torch.cuda.current_device())\n",
    "print(\"Device Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_HOME'] = '/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/amd/zen3/software/CUDA/12.1.1'\n",
    "os.environ['PATH'] = f\"{os.environ['CUDA_HOME']}/bin:{os.environ['PATH']}\"\n",
    "os.environ['LD_LIBRARY_PATH'] = f\"{os.environ['CUDA_HOME']}/lib64:{os.environ.get('LD_LIBRARY_PATH', '')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade  pip\n",
    "%pip install -U  transformers accelerate datasets deepspeed\n",
    "%pip install torch --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "import json\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model name (from huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"stabilityai/stable-code-3b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model and Tokenizer\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 920\n",
      "Eval dataset size: 307\n",
      "Test dataset size: 307\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd94e41c36914b118e76c472c514b8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/307 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"../habrok/dataset.json\")\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "train_eval_dataset = split_dataset[\"train\"]\n",
    "test_dataset = split_dataset[\"test\"]\n",
    "\n",
    "train_eval_split = train_eval_dataset.train_test_split(test_size=0.25)\n",
    "train_dataset = train_eval_split[\"train\"]\n",
    "eval_dataset = train_eval_split[\"test\"]\n",
    "\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Eval dataset size: {len(eval_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "test_dataset.save_to_disk(\"test_dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formating the prompts for the train and eval datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024\n",
    "    )\n",
    "\n",
    "    # \"self-supervised learning\" means the labels are also the inputs:\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "def formatting_prompts_func(datapoint):\n",
    "    question = datapoint[\"question\"]\n",
    "    query = datapoint[\"SQL\"]\n",
    "    database_schema = datapoint[\"database_schema\"]\n",
    "    prompt = f\"\"\"Given the following SQL tables, your job is to generate the Sqlite SQL query given the user's question.\n",
    "Put your answer inside the ⁠```sql and ```⁠ tags.\n",
    "{database_schema}\n",
    "###\n",
    "Question: {question}\n",
    "\n",
    "⁠```sql\n",
    "{query} ;\n",
    "```\n",
    "<|EOT|>\n",
    "\"\"\"\n",
    "\n",
    "    return tokenize(prompt)\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=False)\n",
    "eval_dataset = eval_dataset.map(formatting_prompts_func, batched=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Function\n",
    "\n",
    "We need to prepare the inputs and labels for training. The model expects input in a conversational format.\n",
    "\n",
    "- Messages: We format each example as a conversation between the user and the assistant.\n",
    "- Text Generation: apply_chat_template constructs the conversation text.\n",
    "- Tokenization: We tokenize the full conversation and the assistant’s response separately.\n",
    "- Labels: We set labels to -100 (ignore index) for the input tokens and only compute loss on the assistant’s response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Apply the Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    return_tensors=\"pt\",\n",
    "    pad_to_multiple_of=8,  # Efficient padding for GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Set Up Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=32, # effective batch size\n",
    "    learning_rate=5e-5,\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10,  # Evaluate every 100 steps\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    group_by_length=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pyarrow datasets numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home3/s4787730/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 07:36, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.227900</td>\n",
       "      <td>0.082383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.085400</td>\n",
       "      <td>0.070294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.072600</td>\n",
       "      <td>0.065380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.048300</td>\n",
       "      <td>0.064510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.064372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=56, training_loss=0.09117009490728378, metrics={'train_runtime': 463.3844, 'train_samples_per_second': 3.971, 'train_steps_per_second': 0.121, 'total_flos': 2.936010920951808e+16, 'train_loss': 0.09117009490728378, 'epoch': 1.9478260869565216})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the dataset from the directory where it was saved\n",
    "test_dataset = load_from_disk(\"test_dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "\n",
    "model_name = \"stabilityai/stable-code-3b\"\n",
    "b_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "b_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# List to store results for all entries\n",
    "all_results = []\n",
    "\n",
    "for entry in test_dataset:\n",
    "    question = entry[\"question\"]\n",
    "    query = entry[\"SQL\"]\n",
    "    database_schema = entry[\"database_schema\"]\n",
    "    \n",
    "    # Generate the prompt\n",
    "    prompt = f\"\"\"Given the following SQL tables, your job is to generate the Sqlite SQL query given the user's question.\n",
    "Put your answer inside the ⁠```sql and ```⁠ tags.\n",
    "{database_schema}\n",
    "###\n",
    "Question: {question}\n",
    "\n",
    "⁠```sql\n",
    "\"\"\"\n",
    "    input_text = prompt\n",
    "    inputs = ft_tokenizer(input_text, return_tensors=\"pt\")  # Return PyTorch tensors\n",
    "\n",
    "    # Step 2: Move model and inputs to the same device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    b_model.to(device)  # Move the model to the GPU if available\n",
    "\n",
    "    # Move input tensors to the same device as the model\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    # Step 3: Generate the output\n",
    "    output = b_model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "    # Step 4: Decode the generated tokens into readable text\n",
    "    generated_text = b_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    # Extract all SQL queries using regex and account for the \" ;\" at the end\n",
    "    sql_queries = re.findall(r'```sql\\s+(.*?)\\s+```', generated_text, re.DOTALL)\n",
    "\n",
    "    # Check if there is a second query\n",
    "    if len(sql_queries) >= 2:\n",
    "        second_sql_query = sql_queries[1].rstrip(' ;')  # Get the second query and strip any trailing \" ;\"\n",
    "    else:\n",
    "        second_sql_query = None  # Handle case where there is no second query\n",
    "\n",
    "    # Create the dictionary to store the result for this entry\n",
    "    output_data = {\n",
    "        \"question_id\": entry[\"question_id\"],\n",
    "        \"db_id\": entry[\"db_id\"],\n",
    "        \"Original SQL\": entry[\"SQL\"],\n",
    "        \"Generated SQL\": second_sql_query\n",
    "    }\n",
    "    \n",
    "    # Append the result to the list of all results\n",
    "    all_results.append(output_data)\n",
    "\n",
    "# Save the list of results as a JSON file\n",
    "with open(\"generated_sql_nt_model.json\", \"w\") as json_file:\n",
    "    json.dump(all_results, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
